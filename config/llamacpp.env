# llama.cpp backend configuration
# Copy or source this file before running ktulhuMain to define the default llama settings.

# Path to the llama-cli binary (used to locate libllama/libggml). Uncomment to override.
LLAMA_CLI_BIN=/home/yaro/projects/ktulhu-main/llama.cpp/build/bin/llama-cli

# GGUF checkpoint to load. Defaults to the Ministral instruct GGUF in models/.
LLAMA_CLI_MODEL=/home/yaro/projects/ktulhu-main/models/Ministral3-14B-Resoning-gguf/Ministral-3-14B-Reasoning-2512-Q8_0.gguf

# llama.cpp runtime knobs:
 LLAMA_CLI_CTX=8600           # context window size
 LLAMA_CLI_MAX_TOKENS=4000     # max tokens to sample per reply
 LLAMA_CLI_TEMP=0.70          # temperature
LLAMA_CLI_TOP_P=0.90         # nucleus sampling
LLAMA_CLI_TOP_K=40           # top-k sampling
# LLAMA_CLI_NGL=0              # layers to offload; -1 or unset uses default
 LLAMA_CLI_THREADS=4          # CPU threads; 0 = num physical cores
LLAMA_CLI_CTX_POOL=3
# Optional path to the directory containing libllama.so/libggml*.so if you built elsewhere.
# LLAMA_CPP_LIBDIR=/home/yaro/projects/ktulhu-main/llama.cpp/build/bin
